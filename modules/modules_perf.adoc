= Are modules fast?
:copyright: Copyright 2019 Rene Rivera
:author: Rene Rivera
:email: grafikrobot@gmail.com
:docnumber: D1441
:revnumber: {docnumber}R1
:revdate: {docdate}
:version-label!:
:reproducible:
:nofooter:
:sectanchors:
:sectnums:
:sectnumlevels: 5
:source-highlighter: highlightjs
:source-language: c++
:toc: left
:toclevels: 5
:caution-caption: ⚑
:important-caption: ‼
:note-caption: ℹ
:tip-caption: ☀
:warning-caption: ⚠
:CPP: C++
:PP: ++
:P: +

ifdef::backend-html5[]
++++
<style>
include::std.css[]
</style>
<script src="echarts.min.js"></script>
++++
endif::[]

[horizontal]
Document number:: ISO/IEC/JTC1/SC22/WG21/{revnumber}
Date:: {revdate}
Reply-to:: {author}, {email}
Audience:: WG21

== Abstract

Measurements of the performance of building {CPP} modules and relevant comparisons.

== Changes

=== R1

Fix grammatical errors. Additional data of parallel building from higher thread
and faster hardware, thanks to Robert Maynard for running the tests.

=== R0 (Initial)

Initial performance measurements for synthetic tests of gcc merged modules
implementation (rev 268043, 2019-01-17 10:58:47 -0600 (Thu, 17 Jan 2019)) .

== Introduction

One of the stated goals of the modules proposals was performance over existing
non-modular source specifically in terms of scalable builds. This paper aims to
answer the important question of whether that goal was achieved by current
modules implementations.

Building software does not happen in the solitude of the C++ compiler. It is
a careful orchestration of a collection of tools from the preprocessor,
compiler, linker, assembler, etc controlled by the build system. These have
all been optimized to deal with the current separate compilation model and
generally perform gargantuan feats. But as such it means that performance
measurement needs to take into account, to some degree, all those in a
controlled method to generate meaningful measurements.

To that end the measurements presented here are "synthetic". They are structured
such that they simulate and isolate certain components, like the build system,
to facilitate the extraction of relevance in the data. Such methods are
described below for individual measurements.

== Parallel Build

Although there are various aspects comprising performance scalability of key
interest is in seeing how the potential algorithmic savings of modules "caching"
compiler work at the cost of longer DAG build chains compares against the current,
almost unlimited, parallelized building of plain source compiles. This test aims
to measure the overall compile times of modular source across varied dependency
chain depth against equivalent non-modular source.

=== Method

Overall we perform the basic task of compiling 300 {CPP} source files into
corresponding object files only. We perform the compile at different dependency
DAG chain depth counts. Each source depends, either as an `import` or
`#include`, on some number of sources (headers or modules) from all the
previous DAG levels. The overall time to compile all 300 files is taken
as the result of the measurements.

==== Modular Source

For modular source this test simulates the behavior of a build system to
compile individual modular source files in parallel executions of the
compiler to the limit of the available CPU threads. It does so in
appropriate, but simulated, dependency DAG order; compiling each DAG
level as a group. The process goes as such:

1) Generate a synthetic, and simplified, DAG to describe the build. Where
all the sources in the level can be compiled in parallel.

2) Generate all source files similar to this:

[source]
----
export module m148;

import m0;
import m15;
import m84;

namespace m148_ns
{
export int n = 0;
export int i1 = 1;
// ...
export int i300 = 300;
}
----

Where the imports are randomly chosen from the set of all already compiled
modules (in previous DAG levels).

3) For each DAG level compile all the source files therein with a GCC invocation
similar to:

[source,bash]
----
g++ -fmodules-ts -c -O0 m148.cpp
----

==== Non-modular Source

For no-modular source this test simulates the behavior of a build system to
compile individually all source files in parallel executions of the
compiler to the limit of the available CPU threads. As the non-modular
sources only depend on already existing header source files all source
files can be attempted to be compiled at once. The process goes as such:

1) Generate a synthetic, and simplified, DAG to describe the build with
parity to the modular build. But which is not used in the build itself.

2) Generate all source header files similar to this:

[source]
----
#ifndef H_GUARD_h148
#define H_GUARD_h148
#include "h77.hpp"
#include "h78.hpp"
#include "h92.hpp"


namespace h148_ns
{
int n = 0;
 int i1 = 1;
// ...
 int i300 = 300;
}
#endif
----

Where the includes are randomly chosen from the set of all headers files in
previous DAG levels. Although not needed to limit to previous DAG levels
this is done to keep parity with the modular source in terms of statistical
complexity of the included source size.

3) Generate all source files similar to this:

[source]
----
#include "h148.hpp"
----

4) Compile all source files as one group with a GCC invocation similar to:

[source,bash]
----
g++ -c -O0 h148.cpp
----

=== Limitations

This test has some real-life limitations borne out of the software and hardware
used for testing. Some of these limitations where discovered through
experimentation, for example with internal compiler errors.

The current test hardware is an OSX laptop with an Intel 4 core and 8 thread CPU.
And as such the execution pool for compilation only attempts to execute 8
processes at once.

Being an experimental compiler the support for compiling modules is fragile and
placed severe limits on what the modular sources could contain. Most current
{CPP} constructs fail to compile reliably, for example templates, and cause ICEs.
This is why the generated sources only contain `int` variable definitions.

Along that same line the current modular compiler also seems to have stability
limits on how many imports can be done. This is why only a maximum of three (3)
`import` statements are included in the source.

This is a simulation with "perfect" build knowledge as all dependency and source
information is known before building starts. Hence it is only a very rough
approximation of reality where build system have to deal with dependency
discovery while building and in the face of generated source files.

++++
<div style="page-break-before: always"></div>
++++

=== Results

++++
<script type="text/javascript">
function parallel_build_value_fmt(v) {
    return Number.parseFloat(v).toFixed(1).toString();
}
function create_parallel_build_chart(target, title, data) {
    var chart = echarts.init(document.getElementById(target));
    chart.setOption({
        title: title,
        legend: { data:['Non-Modular', 'Modular'] },
        tooltip: {
            trigger: 'axis',
            axisPointer: { type: 'shadow' }
        },
        xAxis: [ { name: 'DAG Depth', type: 'category', nameLocation: 'center', nameGap: 30 } ],
        yAxis: [ { name: 'Wall Clock Seconds', type: 'value', nameLocation: 'center', nameGap: 45 } ],
        series: [
            {
                name: 'Non-Modular',
                type: 'bar',
                // type: 'pictorialBar', symbol: 'rect',
                label: { normal: {
                    show: true, position: 'top', rotate: 90,
                    align: 'left', verticalAlign: 'middle',
                    formatter: function (params) { return parallel_build_value_fmt(params.value[1]); }
                } },
                encode: { x: 'dag_depth', y: 'headers' },
            },
            {
                name: 'Modular',
                type: 'bar',
                // type: 'pictorialBar', symbol: 'roundRect', symbolRepeat: true, symbolSize: ['100%', 5], barGap: '0%',
                label: { normal: {
                    show: true, position: 'top', rotate: 90,
                    align: 'left', verticalAlign: 'middle',
                    formatter: function (params) { return parallel_build_value_fmt(params.value[2]); }
                } },
                encode: { x: 'dag_depth', y: 'modules' },
                formatter: parallel_build_value_fmt,
            },
        ],
        dataset: { source: data }
    });
}
</script>
++++

++++
<div id="parallel_build" style="height: 400px;"></div>
<script type="text/javascript">
var parallel_build_chart = create_parallel_build_chart(
'parallel_build',
{
    text: 'Jobs: 8',
    subtext: 'Coqui: Intel Mobile Core i7 "Haswell/Crystalwell" (I7-4870HQ) @ 2.5GHz (4 physical cores / 8 threads)'
},
[["dag_depth", "headers", "modules"],
 [3, 4.20, 4.03],
 [32, 17.67, 7.25],
 [61, 19.99, 8.92],
 [90, 20.23, 11.78],
 [119, 20.94, 17.66],
 [148, 18.66, 17.67],
 [177, 18.71, 35.62],
 [206, 18.58, 35.40],
 [235, 19.00, 36.04],
 [264, 16.80, 36.04],
 [293, 18.89, 36.10],
]
);
</script>
++++

As we can see there is an initial performance advantage to modular source
over non-modular source. This advantage diminishes as the dependency depth
grows and hence the number of parallel modular source compiles decreases.
The performance of the non-modular stays about the same throughout as the
amount of header information per source doesn't change substantially.
Whereas the modular build reaches a point where the number of parallel
builds are either one (1) or two (2) executions making the execution
essentially linear.

This indicates that as long as we can execute parallel modular compiles at
the same rate as non-modular compiles it appears that modular compiles have
a varied but perceptible performance advantage.

This also indicates that the larger the number of real parallel builds
possible the less advantage modular compiles have over non-modular compiles.

In the next run we see what using a faster processor does to the differential
of the two builds. First it's an equivalent 8 job run.

++++
<div id="parallel_build_forge_8" style="height: 400px;"></div>
<script type="text/javascript">
var parallel_build_chart = create_parallel_build_chart(
'parallel_build_forge_8',
{
    text: 'Jobs: 8',
    subtext: 'Forge: Intel Xeon W-2145 CPU @ 3.70GHz (8 physical cores / 16 w HT)'
},
[["dag_depth", "headers", "modules"],
[3, 1.0087610629852861, 0.9181149080395699],
[32, 6.219321154989302, 3.495909297140315],
[61, 6.822150422027335, 8.447206597076729],
[90, 6.62159296288155, 11.705184109974653],
[119, 7.020121752982959, 16.304624445037916],
[148, 6.218935227021575, 15.99679002398625],
[177, 6.920910760993138, 31.80375639302656],
[206, 6.7199871190823615, 31.809441711986437],
[235, 7.019685101928189, 32.10625554807484],
[264, 7.523170454893261, 32.21603080490604],
[293, 7.421268946025521, 32.819102976005524],
]
);
</script>
++++

As we can see the difference is now substantially smaller between non-modular
and modular. And the modular build looses it's advantage at a much shorter
DAG depth. We now see data for ever increasing parallel job executions. First
on the same machine but using the extra CPU threads.

++++
<div id="parallel_build_forge_16" style="height: 400px;"></div>
<script type="text/javascript">
var parallel_build_chart = create_parallel_build_chart(
'parallel_build_forge_16',
{
    text: 'Jobs: 16',
    subtext: 'Forge: Intel Xeon W-2145 CPU @ 3.70GHz (8 physical cores / 16 w HT)'
},
[["dag_depth", "headers", "modules"],
[3, 0.7142633509356529, 0.6305585629306734],
[32, 4.42658746894449, 3.6445175728295],
[61, 5.120990126160905, 8.498418937902898],
[90, 4.320516403997317, 11.56579358316958],
[119, 4.519910953938961, 16.905501161934808],
[148, 5.923867805162445, 16.80855169799179],
[177, 5.122375542065129, 34.02343076188117],
[206, 4.521708729909733, 33.73796621104702],
[235, 5.422818426974118, 33.539066845783964],
[264, 5.722841507988051, 33.93446898995899],
[293, 4.9209292840678245, 34.26832155906595],
]
);
</script>
++++

And following with a slower machine but with more threads available. In which
we see almost no advantage of the modular build over the non-modular build.

++++
<div id="parallel_build_adora_20" style="height: 400px;"></div>
<script type="text/javascript">
var parallel_build_chart = create_parallel_build_chart(
'parallel_build_adora_20',
{
    text: 'Jobs: 20',
    subtext: 'Adora: Intel Xeon CPU E5-2640 v4 @ 2.40GHz(Dual Socket 20 physical cores / 40 w HT)'
},
[["dag_depth", "headers", "modules"],
[3, 0.8324654409661889, 0.7814998480025679],
[32, 3.6408212459646165, 4.337540799053386],
[61, 3.8346789749339223, 9.880849721026607],
[90, 3.7342746899230406, 13.396270678029396],
[119, 4.135533166001551, 20.20463444001507],
[148, 3.9350876439129934, 20.0246805800125],
[177, 3.738359506940469, 40.21712113998365],
[206, 3.735614522942342, 39.99239156895783],
[235, 4.337132448912598, 39.88247996591963],
[264, 4.035582971991971, 39.84508032305166],
[293, 4.437921910895966, 40.12975517997984],
]
);
</script>
++++

At this point we have enough available threads that the non-modular build is
always faster than the modular one.

++++
<div id="parallel_build_adora_40" style="height: 400px;"></div>
<script type="text/javascript">
var parallel_build_chart = create_parallel_build_chart(
'parallel_build_adora_40',
{
    text: 'Jobs: 40',
    subtext: 'Adora: Intel Xeon CPU E5-2640 v4 @ 2.40GHz(Dual Socket 20 physical cores / 40 w HT)'
},
[["dag_depth", "headers", "modules"],
[3, 0.5466072140261531, 0.7457621870562434],
[32, 2.660381791065447, 5.159351971000433],
[61, 2.8620165209285915, 12.021816556924023],
[90, 2.6592831240268424, 16.095812772982754],
[119, 3.071641996037215, 24.11021802900359],
[148, 2.7618402070365846, 24.295514196972363],
[177, 2.96113809698727, 48.567696807091124],
[206, 2.960873455973342, 48.52055719902273],
[235, 2.959431970026344, 48.238920859992504],
[264, 2.7622248779516667, 48.05883625894785],
[293, 2.8615603980142623, 48.27250578906387],
]
);
</script>
++++

And finally, as the CPU speed goes lower but the threads goes up the difference
between the two builds is readily apparent. With modular builds loosing
at all levels.

++++
<div id="parallel_build_camelot_68" style="height: 400px;"></div>
<script type="text/javascript">
var parallel_build_chart = create_parallel_build_chart(
'parallel_build_camelot_68',
{
    text: 'Jobs: 68',
    subtext: 'Camelot: Intel Xeon Phi CPU 7250 @ 1.40GHz (68 physical cores / 272 with 4 way SMT)'
},
[["dag_depth", "headers", "modules"],
[3, 1.7595279880333692, 2.1799887949600816],
[32, 8.06883534498047, 18.5961584339384],
[61, 9.096224897075444, 42.67319647106342],
[90, 9.611890556057915, 56.74518109404016],
[119, 9.153583665029146, 85.26279399089981],
[148, 7.369923300924711, 85.27267788292374],
[177, 8.252392338006757, 170.98840467899572],
[206, 9.793684736941941, 170.75660639989655],
[235, 10.23349713603966, 170.82041397399735],
[264, 9.471675020991825, 171.1151053160429],
[293, 9.702734051970765, 170.9213510870468],
]
);
</script>
++++

++++
<div id="parallel_build_camelot_136" style="height: 400px;"></div>
<script type="text/javascript">
var parallel_build_chart = create_parallel_build_chart(
'parallel_build_camelot_136',
{
    text: 'Jobs: 136',
    subtext: 'Camelot: Intel Xeon Phi CPU 7250 @ 1.40GHz (68 physical cores / 272 with 4 way SMT)'
},
[["dag_depth", "headers", "modules"],
[3, 2.0307430600514635, 3.4948218159843236],
[32, 6.0405289239715785, 35.49258812505286],
[61, 6.764967894996516, 81.36800942802802],
[90, 6.280745088937692, 109.36394645192195],
[119, 6.335177005967125, 163.58082887192722],
[148, 6.615980034926906, 163.45939861994702],
[177, 7.076706105028279, 328.559180035023],
[206, 6.492715556989424, 328.74980488594156],
[235, 7.19850400602445, 327.94702022802085],
[264, 6.596547717927024, 328.61911632702686],
[293, 6.832825995050371, 328.309868671],
]
);
</script>
++++

++++
<div id="parallel_build_camelot_272" style="height: 400px;"></div>
<script type="text/javascript">
var parallel_build_chart = create_parallel_build_chart(
'parallel_build_camelot_272',
{
    text: 'Jobs: 272',
    subtext: 'Camelot: Intel Xeon Phi CPU 7250 @ 1.40GHz (68 physical cores / 272 with 4 way SMT)'
},
[["dag_depth", "headers", "modules"],
[3, 3.4972561289323494, 8.472751475055702],
[32, 7.411651192000136, 96.71444478898775],
[61, 7.176073360955343, 222.80584899999667],
[90, 7.527861946960911, 300.56390088098124],
[119, 7.30671163299121, 447.55594796303194],
[148, 8.037210080889054, 449.50167113903444],
[177, 7.704545061104, 900.5817094079684],
[206, 7.564260617014952, 898.9721814970253],
[235, 7.595040827989578, 906.8235456779366],
[264, 7.5456773799378425, 907.7898443009472],
[293, 6.927266972954385, 899.7880558320321],
]
);
</script>
++++

== Conclusion

With the limitations of the capabilities of the compiler wen can only conclude
that modular builds could be advantageous only for slower and low parallelism
environments. In other words, that modules currently do not scale in the same
ways as traditional compilation.

== Acknowledgements

Thanks to Nathan Sidwell for the work to implement the latest merged modules
proposal in GCC. And the citizens of the Internet and more specifically the
contributors to StackOverflow for the hints that made it possible to
decipher how to get the GCC svn checkout to build in OSX.

Special thanks to Robert Maynard for taking the time and resources to build
GCC and run the test script on various machine configurations and job counts.

== References

GCC cxx-modules implementation, Nathan Sidwell
link:https://gcc.gnu.org/wiki/cxx-modules[https://gcc.gnu.org/wiki/cxx-modules]

{CPP} Tooling Stats, Rene Rivera
link:https://github.com/bfgroup/cpp_tooling_stats[https://github.com/bfgroup/cpp_tooling_stats]